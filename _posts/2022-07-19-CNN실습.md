---
layout: single
title: "CNN_ì„±ë³„ ë¶„ë¥˜"
category: DLM
tag: [python, DeepLearning, Tensorflow]
toc: true
toc_sticky: true
---


# í•©ì„±ê³± ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì„±ë³„ ë¶„ë¥˜

## CelebA ë°ì´í„°ì…‹ì— ìˆëŠ” ì–¼êµ´ ì´ë¯¸ì§€ì˜ ì„±ë³„ì„ ë¶„ë¥˜

* 20ë§Œ 2599ê°œì˜ ìœ ëª… ì¸ì‚¬ ì–¼êµ´ ì´ë¯¸ì§€

* ê° ì´ë¯¸ì§€ë§ˆë‹¤ ì–¼êµ´ì— ëŒ€í•œ íŠ¹ì§• 40ê°œë¥¼ True/Falseë¡œ ì œê³µ
  
  * ì„±ë³„(ë‚¨ì„±, ì—¬ì„±), ë‚˜ì´(ì Šì€ì´, ë…¸ì¸)ê°€ í¬í•¨ë˜ì–´ìˆìŒ

* í›ˆë ¨ë°ì´í„°ì˜ ì¼ë¶€(1ë§Œ6000ê°œ)ë§Œ ì‚¬ìš©

* ê³¼ëŒ€ì í•©ì„ ë§‰ê¸°ìœ„í•œ Data Augmentationê¸°ë²• ì‚¬ìš©

### CelebA Dataset Load

```python
import tensorflow as tf
import tensorflow_datasets as tfds
```

```python
## ë°ì´í„° ì…‹ì˜ sampleê°œìˆ˜ë¥¼ ì„¸ëŠ” í•¨ìˆ˜
def count_items(ds):
    n = 0
    for _ in ds:
        n += 1
    return n
```

```python
celeba_bldr = tfds.builder('celeb_a')
celeba_bldr.download_and_prepare()
celeba = celeba_bldr.as_dataset(shuffle_files=False)
celeba_train = celeba['train']
celeba_valid = celeba['validation']
celeba_test = celeba['test']
```

    [1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\tensorflow_datasets\celeb_a\2.0.1...[0m
    
    
    
    Dl Completed...: 0 url [00:00, ? url/s]
    
    
    
    Dl Size...: 0 MiB [00:00, ? MiB/s]
    
    
    
    Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
    
    
    
    Generating train examples...: 0 examples [00:00, ? examples/s]
    
    
    
    Shuffling ~\tensorflow_datasets\celeb_a\2.0.1.incompleteH4A75F\celeb_a-train.tfrecord*...:   0%|          | 0/â€¦
    
    
    
    Generating validation examples...: 0 examples [00:00, ? examples/s]
    
    
    
    Shuffling ~\tensorflow_datasets\celeb_a\2.0.1.incompleteH4A75F\celeb_a-validation.tfrecord*...:   0%|         â€¦
    
    
    
    Generating test examples...: 0 examples [00:00, ? examples/s]
    
    
    
    Shuffling ~\tensorflow_datasets\celeb_a\2.0.1.incompleteH4A75F\celeb_a-test.tfrecord*...:   0%|          | 0/1â€¦
    
    
    [1mDataset celeb_a downloaded and prepared to ~\tensorflow_datasets\celeb_a\2.0.1. Subsequent calls will reuse this data.[0m
    dict_keys([Split('train'), Split('validation'), Split('test')])

ì „ì²´ í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  16,000ê°œì˜ í›ˆë ¨ ìƒ˜í”Œê³¼ ê²€ì¦ìš©ìœ¼ë¡œ 1,000ê°œì˜ ìƒ˜í”Œë§Œ ì‚¬ìš©

```python
celeba_train = celeba_train.take(16000)
celeba_valid = celeba_valid.take(1000)

print('í›ˆë ¨ ë°ì´í„°ì…‹: {}'.format(count_items(celeba_train)))
print('ê²€ì¦ ë°ì´í„°ì…‹: {}'.format(count_items(celeba_valid)))
```

    í›ˆë ¨ ë°ì´í„°ì…‹: 16000
    ê²€ì¦ ë°ì´í„°ì…‹: 1000

### ì´ë¯¸ì§€ ë³€í™˜ê³¼ ë°ì´í„° ì¦ì‹

* ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ê±°ë‚˜ ë’¤ì§‘ê±°ë‚˜ ëŒ€ë¹„, ëª…ë„, ì±„ë„ë¥¼ ë°”ê¿€ ìˆ˜ ìˆë‹¤.
* tf.imageëª¨ë“ˆì„ í™œìš©
1. ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ê¸°
2. ì´ë¯¸ì§€ë¥¼ ìˆ˜í‰ìœ¼ë¡œ ë’¤ì§‘ê¸°
3. ëŒ€ë¹„ ì¡°ì •í•˜ê¸°
4. ëª…ë„ ì¡°ì •í•˜ê¸°
5. ì´ë¯¸ì§€ ì¤‘ì•™ë¶€ë¥¼ ì˜ë¼ì„œ ì›ë³¸ì´ë¯¸ì§€í¬ê¸°(218, 178)ë¡œ í™•ëŒ€í•˜ê¸°

### Data Augmentation

```python
def preprocess(example, size=(64, 64), mode='train'):
    image = example['image']
    label = example['attributes']['Male']
    ## augmentationì€ í•™ìŠµì—ë§Œ ì‚¬ìš©
    if mode == 'train':
        image_cropped = tf.image.random_crop(
            image, size=(178, 178, 3))
        image_resized = tf.image.resize(
            image_cropped, size=size)
        image_flip = tf.image.random_flip_left_right(
            image_resized)
        return (image_flip/255.0, tf.cast(label, tf.int32))

    else:
        image_cropped = tf.image.crop_to_bounding_box(
            image, offset_height=20, offset_width=0,
            target_height=178, target_width=178)
        image_resized = tf.image.resize(
            image_cropped, size=size)
        return (image_resized/255.0, tf.cast(label, tf.int32))
```

```python
import matplotlib.pyplot as plt
tf.random.set_seed(1)

ds = celeba_train.shuffle(1000, reshuffle_each_iteration=False)
ds = ds.take(2).repeat(5)

ds = ds.map(lambda x:preprocess(x, size=(178, 178), mode='train'))

## 2ê°œì˜ ì´ë¯¸ì§€ë¥¼ 5ë²ˆ augmentationí•œ ê²°ê³¼
fig = plt.figure(figsize=(15, 6))
for j,example in enumerate(ds):
    ax = fig.add_subplot(2, 5, j//2+(j%2)*5+1)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.imshow(example[0])

plt.show()
```

![output_14_0.png](../images/2022-07-19-CNNì‹¤ìŠµ/3e8b001e3775067e6a485dafdb7df770b78838ee.png)

```python
import numpy as np
## ë°ì´í„° ì…‹ ì •ì˜
BATCH_SIZE = 32
BUFFER_SIZE = 1000
IMAGE_SIZE = (64, 64)
steps_per_epoch = np.ceil(16000/BATCH_SIZE)

print(steps_per_epoch)
```

    500.0

```python
ds_train = celeba_train.map(lambda x: preprocess(x, size=IMAGE_SIZE, mode='train'))
ds_train = ds_train.shuffle(buffer_size=BUFFER_SIZE).repeat()
ds_train = ds_train.batch(BATCH_SIZE)

ds_valid = celeba_valid.map(lambda x: preprocess(x, size=IMAGE_SIZE, mode='eval'))
ds_valid = ds_valid.shuffle(buffer_size=BUFFER_SIZE)
ds_valid = ds_valid.batch(BATCH_SIZE)
```

### Model

```python
class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()

        self.conv1 = tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu')
        self.pool1 = tf.keras.layers.MaxPool2D((2,2))
        self.drop1 = tf.keras.layers.Dropout(0.5)

        self.conv2 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')
        self.pool2 = tf.keras.layers.MaxPool2D((2,2))
        self.drop2 = tf.keras.layers.Dropout(0.5)

        self.conv3 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')
        self.pool3 = tf.keras.layers.MaxPool2D((2,2))

        self.conv4 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')
        self.pool4 = tf.keras.layers.GlobalAveragePooling2D()

        self.dense1 = tf.keras.layers.Dense(1, activation=None)

    def call(self, input_tensor):
        x = self.conv1(input_tensor)
        x = self.pool1(x)
        x = self.drop1(x)

        x = self.conv2(x)
        x = self.pool2(x)
        x = self.drop2(x)

        x = self.conv3(x)
        x = self.pool3(x)

        x = self.conv4(x)
        x = self.pool4(x)
        return self.dense1(x)
```

```python
tf.random.set_seed(1)
input_shape=(None, 64, 64, 3)
model = MyModel()
model.build(input_shape=input_shape)

model.summary()
```

    Model: "my_model_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_12 (Conv2D)           multiple                  896       
    _________________________________________________________________
    max_pooling2d_9 (MaxPooling2 multiple                  0         
    _________________________________________________________________
    dropout_6 (Dropout)          multiple                  0         
    _________________________________________________________________
    conv2d_13 (Conv2D)           multiple                  18496     
    _________________________________________________________________
    max_pooling2d_10 (MaxPooling multiple                  0         
    _________________________________________________________________
    dropout_7 (Dropout)          multiple                  0         
    _________________________________________________________________
    conv2d_14 (Conv2D)           multiple                  73856     
    _________________________________________________________________
    max_pooling2d_11 (MaxPooling multiple                  0         
    _________________________________________________________________
    conv2d_15 (Conv2D)           multiple                  295168    
    _________________________________________________________________
    global_average_pooling2d_3 ( multiple                  0         
    _________________________________________________________________
    dense_3 (Dense)              multiple                  257       
    =================================================================
    Total params: 388,673
    Trainable params: 388,673
    Non-trainable params: 0
    _________________________________________________________________

#### Global average-pooling

* [batch x 64 x 64 x 8]í¬ê¸°ì˜ íŠ¹ì„±ë§µ -> [batch x 8]ì˜ í¬ê¸°ë¡œ ë³€í•œë‹¤

```python
model.compile(optimizer=tf.keras.optimizers.Adam(),
              ## ë§ˆì§€ë§‰ ì¸µì— activation=Noneë¡œ ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™”ë¥¼ í•˜ì§€ ì•Šì•„ì„œ ëª¨ë¸ì˜ ì¶œë ¥ì´ ë¡œì§“ì´ë‹¤.
              ## from_logitsì„ ì ìš©í•˜ë©´ ì†ì‹¤í•¨ìˆ˜ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì ìš©
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), 
              metrics=['accuracy'])
```

### Training

```python
history = model.fit(ds_train, validation_data=ds_valid, 
                    epochs=30, steps_per_epoch=steps_per_epoch)
```

    Epoch 1/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1714 - accuracy: 0.9237 - val_loss: 0.1210 - val_accuracy: 0.9460
    Epoch 2/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1680 - accuracy: 0.9291 - val_loss: 0.1024 - val_accuracy: 0.9630
    Epoch 3/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1629 - accuracy: 0.9310 - val_loss: 0.1110 - val_accuracy: 0.9470
    Epoch 4/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1615 - accuracy: 0.9308 - val_loss: 0.1113 - val_accuracy: 0.9510
    Epoch 5/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1646 - accuracy: 0.9302 - val_loss: 0.1262 - val_accuracy: 0.9370
    Epoch 6/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1564 - accuracy: 0.9338 - val_loss: 0.1091 - val_accuracy: 0.9630
    Epoch 7/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1548 - accuracy: 0.9333 - val_loss: 0.1177 - val_accuracy: 0.9530
    Epoch 8/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1519 - accuracy: 0.9342 - val_loss: 0.1645 - val_accuracy: 0.9120
    Epoch 9/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1452 - accuracy: 0.9394 - val_loss: 0.1058 - val_accuracy: 0.9540
    Epoch 10/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1482 - accuracy: 0.9361 - val_loss: 0.1169 - val_accuracy: 0.9510
    Epoch 11/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1441 - accuracy: 0.9417 - val_loss: 0.1044 - val_accuracy: 0.9590
    Epoch 12/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1427 - accuracy: 0.9392 - val_loss: 0.1122 - val_accuracy: 0.9490
    Epoch 13/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1409 - accuracy: 0.9404 - val_loss: 0.1073 - val_accuracy: 0.9530
    Epoch 14/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1385 - accuracy: 0.9424 - val_loss: 0.1056 - val_accuracy: 0.9600
    Epoch 15/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1400 - accuracy: 0.9421 - val_loss: 0.1142 - val_accuracy: 0.9490
    Epoch 16/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1337 - accuracy: 0.9423 - val_loss: 0.0989 - val_accuracy: 0.9650
    Epoch 17/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1389 - accuracy: 0.9414 - val_loss: 0.1052 - val_accuracy: 0.9590
    Epoch 18/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1320 - accuracy: 0.9438 - val_loss: 0.0926 - val_accuracy: 0.9710
    Epoch 19/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1290 - accuracy: 0.9476 - val_loss: 0.1000 - val_accuracy: 0.9610
    Epoch 20/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1276 - accuracy: 0.9446 - val_loss: 0.1070 - val_accuracy: 0.9600
    Epoch 21/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1301 - accuracy: 0.9446 - val_loss: 0.1237 - val_accuracy: 0.9450
    Epoch 22/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1287 - accuracy: 0.9454 - val_loss: 0.0941 - val_accuracy: 0.9670
    Epoch 23/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1251 - accuracy: 0.9492 - val_loss: 0.1041 - val_accuracy: 0.9510
    Epoch 24/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1227 - accuracy: 0.9482 - val_loss: 0.0963 - val_accuracy: 0.9660
    Epoch 25/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1249 - accuracy: 0.9473 - val_loss: 0.0979 - val_accuracy: 0.9640
    Epoch 26/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1238 - accuracy: 0.9486 - val_loss: 0.0967 - val_accuracy: 0.9570
    Epoch 27/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1188 - accuracy: 0.9514 - val_loss: 0.1021 - val_accuracy: 0.9580
    Epoch 28/30
    500/500 [==============================] - 7s 15ms/step - loss: 0.1207 - accuracy: 0.9499 - val_loss: 0.1102 - val_accuracy: 0.9600
    Epoch 29/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1167 - accuracy: 0.9517 - val_loss: 0.0992 - val_accuracy: 0.9640
    Epoch 30/30
    500/500 [==============================] - 8s 15ms/step - loss: 0.1184 - accuracy: 0.9507 - val_loss: 0.1023 - val_accuracy: 0.9690

```python
hist = history.history
x_arr = np.arange(len(hist['loss'])) + 1

fig = plt.figure(figsize=(12, 4))
ax = fig.add_subplot(1, 2, 1)
ax.plot(x_arr, hist['loss'], '-o', label='Train loss')
ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')
ax.legend(fontsize=15)
ax.set_xlabel('Epoch', size=15)
ax.set_ylabel('Loss', size=15)

ax = fig.add_subplot(1, 2, 2)
ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')
ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')
ax.legend(fontsize=15)
ax.set_xlabel('Epoch', size=15)
ax.set_ylabel('Accuracy', size=15)

plt.show()
```

![output_24_0.png](../images/2022-07-19-CNNì‹¤ìŠµ/90fac110c5bb582280aa908018a2f80292096a12.png)

### Test

```python
ds_test = celeba_test.map(
    lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)
results = model.evaluate(ds_test, verbose=0)
print('í…ŒìŠ¤íŠ¸ ì •í™•ë„: {:.2f}%'.format(results[1]*100))
```

    í…ŒìŠ¤íŠ¸ ì •í™•ë„: 96.04%

```python
ds = ds_test.unbatch().take(10) ## unbatchë¥¼ í•´ì•¼ take()ì„ í–ˆì„ ë•Œ 32ê°œê°€ ì•„ë‹Œ 10ê°œë¥¼ ê°€ì ¸ì˜´

pred_logits = model.predict(ds.batch(10))
probas = tf.sigmoid(pred_logits) ## sigmoidë¥¼ ì´ìš©í•´ classí™•ë¥  ê³„ì‚°
probas = probas.numpy().flatten()*100

fig = plt.figure(figsize=(15,7))
for j, example in enumerate(ds):
    ax = fig.add_subplot(2, 5, j+1)
    ax.set_xticks([]); ax.set_yticks([])
    ax.imshow(example[0])
    if example[1].numpy() == 1:
        label='Male'
    else:
        label='Female'
    ax.text(0.5, -0.15, 'GT: {:s}\nPr(Male)={:.0f}%'.format(label, probas[j]), size=16,
           horizontalalignment='center',
           verticalalignment='center',
           transform=ax.transAxes)

plt.show()
```

![output_27_0.png](../images/2022-07-19-CNNì‹¤ìŠµ/21f91a50e5a01c4bf69677665141e780f00ae90d.png)

### PLUS

* ë“œë¡­ì•„ì›ƒí™•ë¥ ê³¼ í•©ì„±ê³± ì¸µì˜ í•„í„° ê°œìˆ˜ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ globalaveragepoolingì„ flattenìœ¼ë¡œ ë°”ê¿€ìˆ˜ë„ìˆë‹¤.
* ì°¸ê³ : ë¨¸ì‹ ëŸ¬ë‹ êµê³¼ì„œ with íŒŒì´ì¬, ì‚¬ì´í‚·ëŸ°, í…ì„œí”Œë¡œ ê°œì •3íŒ